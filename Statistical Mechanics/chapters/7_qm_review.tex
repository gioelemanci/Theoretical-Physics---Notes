\chapter{QM and Statistics}\TODO{ADD HAT TO OPERATORS THROUGHOUT THE CHAPTER}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.40\textwidth}
        \vspace{0pt}
        The starting point is a classical system with a finite number of degrees of freedom, from which we can either move to a classical field theory with infinite degrees of freedom (with the idea to quantize after it), or to a first quantization of our finite classical system (with the idea to extend observables to field later). It's not trivial that the result is the same taking both procedures, but it's true.
    \end{minipage}
    \hfill
    \begin{minipage}{0.55\textwidth}
        \resizebox{\textwidth}{!}{
            \begin{tikzpicture}[
                    % Node styles
                    box/.style={
                            draw, % Draw border
                            fill=blue!10, % Light fill color
                            rounded corners, % Rounded corners
                            minimum height=1.2cm, % Minimum height
                            minimum width=3.5cm, % Minimum width
                            text width=3.2cm, % Text width
                            align=center, % Text alignment
                            font=\bfseries % Bold font
                        },
                    % Arrow style
                    arrow/.style={
                            ->, % Forward arrow
                            >=stealth, % Tip style
                            very thick, % Thickness
                            draw=black!70 % Color
                        },
                    % Arrow label style
                    label/.style={
                            midway, % Midpoint of the line
                            font=\footnotesize, % Small font
                            fill=white, % White background
                            text=red!80!black % Text color
                        }
                ]

                % Node Definitions
                % 1. Starting Point (Bottom Left)
                \node[box, fill=green!20] (CM) {Classical Mechanics};

                % 2. Intermediate Nodes (Two Paths)
                % 2a. Field (Top Left)
                \node[box, fill=yellow!20, above right=1.5cm and -0.5cm of CM] (CFT) {Classical Field Theory};
                % 2b. Quantization (Bottom Right)
                \node[box, fill=yellow!20, below right=1.5cm and -0.5cm of CM] (QM) {Quantum Mechanics};

                % 3. Endpoint (Top Right)
                \node[box, fill=blue!20, right=2.5cm of CM, yshift=0.0cm] (QFT) {Quantum Field Theory};

                % Connections and Arrows

                % Path 1: Classical Mechanics -> Classical Field Theory -> QFT
                % CM -> CFT
                \draw[arrow] (CM.north) -- (CFT.west) node[label, xshift=0.2cm] {Generalize to Fields}; % Modificato xshift
                % CFT -> QFT
                \draw[arrow] (CFT.east) -- (QFT.north) node[label, yshift=0.1cm] {Quantize (2\textsuperscript{nd} time)}; % Modificato yshift

                % Path 2: Classical Mechanics -> Quantum Mechanics -> QFT
                % CM -> QM
                \draw[arrow] (CM.south) -- (QM.west) node[label, xshift=0.2cm] {Quantize (1\textsuperscript{st} time)}; % Modificato xshift
                % QM -> QFT
                \draw[arrow] (QM.east) -- (QFT.south) node[label, yshift=-0.1cm] {Generalize to Fields}; % Modificato yshift

            \end{tikzpicture}
        }
    \end{minipage}
    \label{fig:qft_paths}
\end{figure}

Let's briefly review quantum mechanics, in comparison with a classical system as a start.

\begin{table}[H]
    \centering
    \label{tab:cm_qm_comparison_en}
    \begin{tabular}{l c l}
        \toprule
        \textbf{Classical Mechanics (CM)} &                         & \textbf{Quantum Mechanics (QM)}                                                                                                                                                                                                                                                                                                                                                            \\
        \midrule
        \begin{minipage}{5cm}
            A precise point in the phase space $(\mathbf{q}, \mathbf{p})$, where position and momentum are simultaneously determined.
        \end{minipage}
                                          & \textbf{State}          &
        \begin{minipage}{5cm}
            A normalized state vector $\ket{\psi} \in \mathcal{H}$ (Hilbert Space), defined up to a global phase: equivalence classes called \textbf{rays}.
        \end{minipage}                                                                                                                                                                                                                                                                                    \\
        \midrule
        \begin{minipage}{5cm}
            Functions defined on the phase space, e.g. $f(\mathbf{q}, \mathbf{p}) \in \mathbb{R}$.
        \end{minipage}
                                          & \textbf{Observables}    &
        \begin{minipage}{5cm}
            Linear, self-adjoint (Hermitian) operators $\hat{A} = \hat{A}^{\dagger}$ acting on the Hilbert space.
        \end{minipage}                                                                                                                                                                                                                                                                                                                                 \\
        \midrule
        \begin{minipage}{5cm}
            Direct and simultaneous. The act of measurement does not alter the system's state. We can measure a value of the observable \(f(\mathbf{q}^o, \mathbf{p}^o)\) at a specific point \((\mathbf{q}^o, \mathbf{p}^o)\).
        \end{minipage}
                                          & \textbf{Measurements}   &
        \begin{minipage}{5cm}
            The result is a spectrum of eigenvalues $\lambda_n$ of $\hat{A}$. The state instantly collapses to the corresponding eigenstate $\ket{\lambda_n}$. The probability of \(\lambda_n\) as an outcome is \(P_n = \vert \langle \psi \vert \psi_n \rangle \vert^2 = \bra{\phi}\mathbb{P}_n \ket{\phi}\), if \(\ket{\phi}\) is the generic state and \(\ket{\psi_n}\) is the eigenstate \(\hat{A}\ket{\psi_n} = \lambda_n \ket{\psi_n}\).
        \end{minipage} \\
        \midrule
        \begin{minipage}{5cm}
            Determined by Hamilton's equations \(\dot{q}_i = -\frac{\partial H}{\partial p_i}\) and \(\dot{p}_i = \frac{\partial H}{\partial q_i}\). Evolving the system, the observables can be measured in the new points on the phase space.
        \end{minipage}
                                          & \textbf{Time Evolution} &
        \begin{minipage}{5cm}
            Determined by the Schrödinger equation for the states
            $i\hbar \frac{\partial}{\partial t}\ket{\psi} = \hat{H}\ket{\psi}$.
            Unitary time evolution operator, defined by the Hamiltonian.
        \end{minipage}                                                                                                                                                                                                                                                                                                                                                     \\
        \bottomrule
    \end{tabular}
\end{table}

In more detail, the degrees of freedom are described in terms of a vector in the hilbert space \(\ket{\psi},\,ket{\phi},\,\dots \in \mathcal{H}\), such that
\[
    \alpha \ket{\psi} + \beta\ket{\phi} \in \mathcal{H}, \text{ and } \langle \psi \vert \phi \rangle \in \mathbb{C}.
\]

A \textbf{pure state} is a \textit{ray} belonging to an \textbf{equivalence class}, defined up to a global phase
\[
    \ket{\psi} \sim e^{i \theta} \ket{\psi},\quad \langle \psi \vert \psi \rangle = 1.
\]

An \textbf{observable} is given by a \textit{self-adjoint} operator \(\hat{A} = \hat{A}^{\dagger}\), for which the \textit{spectral theorem} holds
\[
    \hat{A} \ket{\psi_j} = \lambda_j \ket{\psi_j}, \text{ where } \lambda_j \in \mathbb{R},
\]
since the outcome of a measure, the eigenvalues of the interesting observable, mast be real.

\paragraph{Projectors.} It is very useful to introduce operators which can \textit{project} a given state, an arbitrary vector, in the direction spanned by a chosen vector:
\[
    \mathbb{P}_{\psi} = \ket{\psi}\bra{\psi} = \begin{pmatrix}
        a_1   \\
        a_2   \\
        \dots \\
        a_n
    \end{pmatrix}\begin{pmatrix}
        a_1^* & a_2^* & \dots & a_n^*
    \end{pmatrix} = (n\times n) \text{ matrix}.
\]
These \textbf{projectors} have to be \textit{idempotent} matrices, such that
\[
    \mathbb{P}^{\dagger} = \mathbb{P}, \quad \mathbb{P}^2 = \mathbb{P}.
\]
They project vectors onto the \(1D\) Hilbert space spanned by their defining vector. Considering an orthonormal basis \(\{\ket{\psi_n}\}\), we can exploit some interesting properties:
\[
    \mathbb{P}_n\mathbb{P}_m = \ket{\psi_n} \langle\psi_n\vert\psi_m\rangle \bra{\psi_n} = \ket{\psi_n}\bra{\psi_n} \delta_{nm},
\]
hence if \(m\neq n\) then the two defining vectors are orthonormal and \(\mathbb{P}_n\mathbb{P}_m = 0\).
If we sum the projectors, i.d. we apply them consequently, we find intersesting compisitions:
\begin{itemize}
    \item \(\mathbb{P}_1 + \mathbb{P}_2\) \(2D\) projector on the plane spanned by \(\{\ket{\psi_1},\,\ket{\psi_2}\}\);
    \item \(\mathbb{P}_1 + \mathbb{P}_2 + \mathbb{P}_3\) \(3D\) projector on the volume spanned by \(\{\ket{\psi_1},\,\ket{\psi_2},\,\ket{\psi_3}\}\);
    \item \(\mathbb{P}_1 + \dots + \mathbb{P}_n\) \(nD\) projector that coincides with the identity:
          \[
              \sum_{n} \mathbb{P}_n = \sum_{n} \ket{\psi_n}\bra{\psi_n} = \mathbb{I}_n.
          \]
\end{itemize}
According to the \textit{spectral theorem}, any hermitian operator can be decomposed in:
\[
    \hat{A} = \sum_{n} \lambda_n \mathbb{P}_n = \sum_{n} \lambda_n \ket{\psi_n}\bra{\psi_n}.
\]

\paragraph{Measure.}
A measure of an observable \(A\) on a state \(\ket{\psi}\) yields a set of possible outcomes \(\lambda_n\) wheighted by a probability \(p_n\) given by
\[
    \ket{\psi}  = \sum_{n} c_n \ket{\psi_n} \quad \implies \quad p_n = \vert c_n \vert^2.
\]
The average value of a measure is given by
\[
    \langle A \rangle = \sum_{n} \lambda_n p_n = \bra{\psi} A \ket{\psi},
\]
with associated standard deviation
\[
    (\Delta A)^2 = \langle A^2 \rangle - \langle A \rangle^2.
\]
One important feature of the measure is that it represents a \textit{destructive process}: once measured, the state \(\ket{\psi}\) collapses into a state in the eigenspace aassociated to the eigenvalue which is the outcome of the measure.

\paragraph{Time evolution.}
The time evolution of the system is governed by a special observable, the Hamiltonian; from Schrödinger equation we deeduce the time evolution of each state:
\[
    i \hbar \frac{\partial}{\partial t} \ket{\psi} = \hat{H} \ket{\psi}.
\]
We can also create an operator responsible for the active implementation of this equation, assuming the time independence of the Hamiltonian, which results to be a unitary operator:
\[
    \ket{\psi(t)} = U(t)\ket{\psi(0)}, \quad U(t) = e^{\tfrac{i}{\hbar} \hat{H} t}.
\]

\section{Density Matrix}

\subsection{Pure State}

We are going to introduce one of the most powerful tools in the quantum description of statistical mechanics, the \textbf{density matrix}.

A pure density matrix \(\rho\) has the following properties:
\begin{itemize}
    \item it is a bounded operator : \(\Vert \rho \Vert \leq 1\);
    \item it is self-adjoint: \(\rho^{\dagger} = \rho\);
    \item it is a positive operator: \(\bra{\alpha} \rho \ket{\alpha} \geq 0 \text{ } \forall \ket{\alpha} \in \mathcal{H}\);
    \item it has unitary trace: \(\Tr(\rho)=1\);
    \item it is idempotent: \(\rho^2 = \rho\).
\end{itemize}

For a pure state, unambiguously defined by a ray \(\ket{\psi}\), the density matrix acquire the meaning of projector onto the subspace spanned by the state:
\[
    \rho_{\psi} = \ket{\psi} \bra{\psi}.
\]
We can use it to define \textbf{expextation values} for operators, which can be written in terms of the density matrix as
\[
    \langle A \rangle = \bra{\psi} A \ket{\psi} = \Tr(A \rho_\psi),
\]
where the trace of an operator with elements \(A_{mn} = \bra{\psi_m} A \ket{\psi_n}\) is given by \(\Tr(A) = \sum_{n} \bra{\psi_n} A \ket{\psi_n} = \sum_{n} A_{nn}\). A trace-class operator \(B\) is defined as, given any ON basis \(\{\ket{e_j}\}\)
\[
    \Tr(B) = \sum_{n} \bra{e_n} A \ket{e_n} < \infty,
\]
i.e. the class of operators with finite trace (for an infinite dimensional operators is absolutely non trivial to have finite trace, even the simplest one, the identity matrix, has a diverging trace).

Let's show the unitariety of the density operator trace: given any ON basis \(\{\ket{e_j}\}\)
\[
    \begin{aligned}
        \Tr(\rho) & = \sum_{n} \bra{e_n} \rho \ket{e_n} = \sum_{n} \bra{e_n} \ket{\psi} \bra{\psi} \ket{e_n}                 \\
                  & = \bra{\psi} \left(\sum_{n} \ket{e_n}\bra{e_n}\right) \ket{\psi} = \bra{\psi} \mathbb{I} \ket{\psi} = 1.
    \end{aligned}
\]

\subsection{Mixed State}

The pure state, as we said, is unambiguously defined by a single ray \(\ket{\psi}\), but it might happen to encounter a system prepared into a ensamble of states \(\{\ket{\psi_{\alpha}},\,p_{\alpha}\}_{\alpha}\), where \(p_{\alpha}\) is the probability of finding the system in the state \(\ket{\psi_{\alpha}}\) and obviously they respect: \(0 \leq p_{\alpha} \leq 1\), \(\sum_{\alpha}p_{\alpha} = 1\).

The density matrix of a classical mixture is given by
\[
    \rho = \sum_{\alpha} p_{\alpha} \rho_{\alpha} = \sum_{\alpha} p_{\alpha}\ket{\alpha} \bra{\alpha},
\]
where \(\rho_{\alpha}\) is the density matrix of the pure state \(\ket{\psi_{\alpha}}\).

\begin{example}
    Consider an experiment using thousands of electrons, where each electron can be in one of two orthogonal spin states along the $z$-axis:
    \begin{enumerate}
        \item \textbf{Pure State 1}: Spin up, $\ket{\psi_1} = \ket{\uparrow}$.
        \item \textbf{Pure State 2}: Spin down, $\ket{\psi_2} = \ket{\downarrow}$.
    \end{enumerate}
    If the electron beam is unpolarized (not specially prepared), the overall system is a statistical mixture described by the ensemble:
    \[
        \left\{ \ket{\uparrow},\, p_1=1/2 \right\} \quad \text{and} \quad \left\{ \ket{\downarrow},\, p_2=1/2 \right\}
    \]
    This collection of electrons is thus described not by a single $\ket{\psi}$ (pure state), but by the density operator (mixed state):
    \[
        \rho = \sum_{\alpha} p_{\alpha} \ket{\psi_{\alpha}}\bra{\psi_{\alpha}} = \frac{1}{2} \ket{\uparrow}\bra{\uparrow} + \frac{1}{2} \ket{\downarrow}\bra{\downarrow}
    \]
\end{example}

The density matrix \(\rho\) of a mixture has the following properties:
\begin{itemize}
    \item it is a bounded operator : \(\Vert \rho \Vert \leq 1\);
    \item it is self-adjoint: \(\rho^{\dagger} = \rho\);
    \item it is a positive operator: \(\bra{\alpha} \rho \ket{\alpha} \geq 0 \text{ } \forall \ket{\alpha} \in \mathcal{H}\);
    \item it has unitary trace: \(\Tr(\rho)=1\);
\end{itemize}
but notice how it is not idempotent anymore. Let us analyze further those properties:
\begin{itemize}
    \item Self adjoint operator:
          \[
              \rho^{\dagger} = \left( \sum_{\alpha} p_{\alpha} \ket{\psi_{\alpha}}\bra{\psi_{\alpha}} \right)^{\dagger} = \sum_{\alpha} p_{\alpha} \bra{\psi^{\dagger}_{\alpha}} \ket{\psi^{\dagger}_{\alpha}} = \sum_{\alpha} p_{\alpha} \ket{\psi_{\alpha}}\bra{\psi_{\alpha}} = \rho.
          \]
    \item Positive operator:
          \[
              \bra{\psi}\rho\ket{\psi} = \bra{\psi}\left(\sum_{\alpha} p_{\alpha} \ket{\psi_{\alpha}}\bra{\psi_{\alpha}}\right) \ket{\psi} = \sum_{\alpha} p_{\alpha} \Vert \bra{\psi_{\alpha}}\ket{\psi} \Vert^2 \geq 0.
          \]
    \item Unitary trace:
          \[
              \begin{aligned}
                  \Tr(\rho) & = \sum_{n} \bra{e_n}\rho\ket{e_n} = \sum_{n} \bra{e_n}\left(\sum_{\alpha} p_{\alpha} \ket{\psi_{\alpha}}\bra{\psi_{\alpha}}\right)\ket{e_n}                                            \\
                            & =\sum_{\alpha n} p_{\alpha} \bra{\psi_{\alpha}}\ket{e_n}\bra{e_n}\ket{\psi_{\alpha}} = \sum_{\alpha} p_{\alpha} \bra{\psi_{\alpha}}\ket{\psi_{\alpha}} = \sum_{\alpha} p_{\alpha} = 1.
              \end{aligned}
          \]
\end{itemize}

Let us show and demonstrate an important theorem:
\begin{theorem}
    If the density matrix is idempotent, then it refers to a pure state:
    \[
        \rho^2 = \rho \iff \rho = \ket{\psi}\bra{\psi}\text{ with } \ket{\psi} \text{ pure.}
    \]
\end{theorem}
\begin{proof}
    \(\impliedby)\) trivial, by definition of density matrix of a pure state.
    \(\implies)\) \(\rho\) has to satisfy at least the four properties reported above for a density matrix. Then if we apply the spectral theorem we can write
    \[
        \rho = \sum_{n} \lambda_n \ket{e_n}\bra{e_n}, \quad \{\ket{e_n}\} \text{ any ON basis},
    \]
    and this implies\QUESTION{What happened? We multiplied both side for what?}
    \[
        \rho \ket{e_n} = \sum_{n} \lambda_n \ket{e_n}.
    \]
    Since \(\rho\) is positive, we know \(\lambda_n \geq 0\), and since it has unitary trace, we can write
    \[
        \Tr(\rho) = 1 \quad \iff \quad 0 \geq \lambda_n \leq 1.
    \]
    If we now compute \(\rho^2\), we get
    \[
        \begin{aligned}
            \rho^2 & = \left( \sum_{n} \lambda_n \ket{e_n}\bra{e_n} \right) \left( \sum_{m} \lambda_m \ket{e_m}\bra{e_m} \right)                                                                   \\
                   & = \sum_{nm} \lambda_n \lambda_m \ket{e_n}\bra{e_n}\ket{e_m}\bra{e_m} = \sum_{nm} \lambda_n \lambda_m \ket{e_n}\delta_{nm}\bra{e_m} = \sum_{n} \lambda_n^2 \ket{e_n}\bra{e_n},
        \end{aligned}
    \]
    and by imposing \(\rho^2 = \rho\) we find out that
    \[
        \rho^2 = \rho \quad \implies \quad \sum_{n} \lambda_n^2 \ket{e_n}\bra{e_n} = \sum_{n} \lambda_n \ket{e_n}\bra{e_n}
    \]
    hence \(\lambda_n^2 = \lambda_n\), which admits \(\lambda_n = 0,\,1\).
    But since \(\sum_{j} \lambda_j = 1\), then the solution imply one \(\lambda_j = 1\) and \(\lambda_i = 0\) for all other indices \(i \neq j\). But if only one eigenvalue survives, then we can only deduce it is a pure density matrix:
    \[
        \rho = \sum_{n} \lambda_n \ket{e_n}\bra{e_n} = \lambda_j \ket{e_j}\bra{e_j} = \ket{e_j}\bra{e_j}.
    \]
\end{proof}

\section{Qubit}

The qubit is a quantum system with only two available states: \(\ket{0}\) and \(\ket{1}\). If we consider the following Hilbert space
\[
    \mathcal{H} = \mathbb{C}^2 = \left\{ \begin{pmatrix}
        \alpha \\
        \beta
    \end{pmatrix}; \quad \alpha,\,\beta \in \mathbb{C} \right\}
\]
and the two states \(\ket{0} = \begin{pmatrix}
    1 \\
    0
\end{pmatrix}\) and \(\ket{1} = \begin{pmatrix}
    0 \\
    1
\end{pmatrix}\), they respect
\[
    \bra{0}\ket{0} = \bra{1}\ket{1} = 1, \text{ and } \bra{1}\ket{0} = \bra{0}\ket{1} = 0.
\]
Then any vector in the hilbert space can be written as a linear combination of these two states:
\[
    \ket{\psi} = \alpha\ket{0}+\beta\ket{1} = \begin{pmatrix}
        \alpha \\
        \beta
    \end{pmatrix}, \quad \vert \alpha \vert^2 + \vert \beta \vert^2 = 1,
\]
for all \(\ket{\psi} \in \mathcal{H}\). Let's notice that \(p_0 = \vert \alpha \vert^2 \) and \(p_1 = \vert \beta \vert^2 \), since they are the weight of the state with respect to \(\ket{0}\) and \(\ket{1}\): they are the probabilities to get an outcome of \(\ket{0}\) or \(\ket{1}\) from a measure on \(\ket{\psi}\). Thus we call \(\ket{\psi}\) a \textbf{quantum mixture}, which is a pure state. Its density matrix is defined as:
\[
    \rho = \ket{\psi}\bra{\psi} = \begin{pmatrix}
        \alpha \\
        \beta
    \end{pmatrix}\begin{pmatrix} \alpha^* & \beta^* \end{pmatrix} = \begin{pmatrix}
        \vert \alpha \vert^2 & \alpha\beta^*       \\
        \beta \alpha^*       & \vert \beta \vert^2
    \end{pmatrix},
\]
where the off-diagonal elements are proportional to a phase \(\alpha\beta^*\), making them responsble for \textbf{interference} effect between the two states.

If we instead prepare from the beginning a state \(\ket{\psi}\) with the same probabilities as before (\(p_0 = \vert \alpha \vert^2\) and \(p_1 = \vert \beta \vert^2\)) but not related to expectation values of observables when measured on \(\ket{\psi}\), but direct probabilities that the system will be either in \(\ket{0}\) or \(\ket{1}\), then we have a \textbf{classical mixture}.
Its density matrix is defined by
\[
    \rho = \vert \alpha \vert^2 \ket{0}\bra{0} + \vert \beta \vert^2 \ket{1}\bra{1} = \vert \alpha \vert^2\begin{pmatrix}
        1 & 0 \\
        0 & 0
    \end{pmatrix} + \vert \beta \vert^2\begin{pmatrix}
        0 & 0 \\
        0 & 1
    \end{pmatrix} = \begin{pmatrix}
        \vert \alpha \vert^2 & 0                   \\
        0                    & \vert \beta \vert^2
    \end{pmatrix}.
\]

If we compare the two expression we can find interesting that the diagonal is the same, while the off diagonal elements are not trivial: while the classical mixture has null off diagonal elements (respectiing a classical probability distribution without interactions among the two states), the quantum mixture presents terms correlated to \textbf{interference} in the measures among the two states.

\paragraph{Observables and expectation values.}
Lets consider a quantum system in a pure state \(\ket{\psi}\); an observable \(A\) (which we remember to be hermitian) will have an expectation value given by
\[
    \langle A \rangle_{\psi} = \bra{\psi} A \ket{\psi} = \Tr(A \rho_{\psi}).
\]
Let us show that the last equation holds:
\[
    \begin{aligned}
        \langle A \rangle_{\psi} & = \Tr(A \rho_{\psi}) = \sum_{n} \bra{e_n} A \rho_{\psi} \ket{e_n}                                             \\
                                 & = \sum_{n} \bra{e_n} \ket{\psi} \bra{\psi} A \ket{e_n} = \sum_{n} \bra{\psi} A \ket{e_n} \bra{e_n} \ket{\psi} \\
                                 & = \bra{\psi} A \sum_{n} \ket{e_n} \bra{e_n} \ket{\psi} = \bra{\psi} A \ket{\psi} = \langle A \rangle_{\psi}.
    \end{aligned}
\]
This expression is very useful because it can be generalized to treat expectation values of observables acting on mixed states. The density matrix for a mixed state \(\ket{\psi}\) is
\[
    \rho_{\psi}  = \sum_{\alpha} p_{\alpha} \rho_{\alpha} = \sum_{\alpha} p_{\alpha}\ket{\psi_\alpha}\bra{\psi_\alpha},
\]
then if we compute the expectation value of \(A\):
\[
    \begin{aligned}
        \langle A \rangle_{\psi} & = \Tr(A \rho_{\psi}) = \Tr\left(\sum_{\alpha} p_{\alpha} \rho_{\alpha} A \right) = \sum_{\alpha} p_{\alpha} \Tr(\rho_{\alpha} A) = \sum_{\alpha} p_{\alpha} \langle A \rangle_{\alpha},
    \end{aligned}
\]
which is the sum of the expectations value of \(A\) in each of the pure states, weighted by the probability of the mixed state to be observed in that state.\footnote{A clearer but heavier notation would have called \(\rho_{\alpha} \to \rho_{\psi_{\alpha}}\) and \(\langle A \rangle_{\alpha} \to \langle A \rangle_{\psi_\alpha}\).}

\section{Identical Particles}

A system of \(N\) particles, each of which are described by a subsystem and its Hilbert space \(\mathcal{H}_j\), lives in the bigger hilbert space defined by
\[
    \mathcal{H} = \mathcal{H}_1 \times \dots \times \mathcal{H}_N.
\]
We can find a base of our system by composing the basis of each subsystem:
\[
    \mathcal{H}_j \longrightarrow \{\ket{e_{k_j}}\}_{k_j} \quad \forall k_j = 1, \dots, \dim(\mathcal{H}_j),
\]
so that the general base will take form
\[
    \mathcal{H} \longrightarrow \{\ket{e_{k_1}},\, \dots ,\, \ket{e_{k_N}}\}_{k_1,\,\dots,\,k_N} \quad \forall k_j = 1, \dots, \dim(\mathcal{H}_j).
\]
Now we can write a generic state (vector) in coordinates of this base as
\[
    \ket{\psi} = \sum_{k_1,\,\dots,\,k_N} \alpha_{k_1,\,\dots,\,k_N} \ket{e_{k_1}} \dots \ket{e_{k_N}}.
\]
The dimensionality of this space will be given by the product of the dimensionalities of the subsystems
\[
    \dim(\mathcal{H}) = \dim(\mathcal{H}_1)\dots\dim(\mathcal{H}_N),
\]
thus the dimensionality of the system grows really fast with the number of particles considered.

Note that if the particles are identical, then
\[
    \mathcal{H} = (\mathcal{H}_1)^{\otimes N}, \quad \dim(\mathcal{H}) = \dim(\mathcal{H}_1)^N.
\]
The system of \textbf{indistinguishable} particles acquire also an important property: it should be invariant under transformations of the \textbf{permutation group}, up to a phase.
If we take as an example a system of two indistinguishable classical particles, then the phase space \((\mathbf{q}_1,\,\mathbf{q}_2,\,\mathbf{p}_1,\,\mathbf{p}_2)\) should be invariant under the exchange of the particles:
\[
    (\mathbf{q}_1,\,\mathbf{q}_2,\,\mathbf{p}_1,\,\mathbf{p}_2) = (\mathbf{q}_2,\,\mathbf{q}_1,\,\mathbf{p}_2,\,\mathbf{p}_1).
\]
Thus a system of \(N\) identicle particles should be invariant under a set of \(N!\) permutations, with the states spanning only a subspace of the bigger Hilbert space just described.

To clarify further, let's take two particles \(e\) and \(f\), both of which can be observed in the state \(\ket{}_1\) or \(\ket{}_2\): we have two possible global states
\[
    \ket{e}_1 \ket{f}_2 \quad \text{or} \quad \ket{f}_1 \ket{e}_2.
\]
But if we consider the case in which the two particles are identical \(e \equiv f\) or the two states are the same \(\ket{e}_1 \equiv \ket{e}_2\), then the system can be written in thw previous two ways, but now they are completely undistinguishable, i.e. a single state:
\[
    \ket{e}_1 \ket{f}_2 \equiv \ket{f}_1 \ket{e}_2.
\]

So if a state of \(N\) particles living in \(\ket{\psi} \in \mathcal{H}\) and it is left invariant by any of the \(N!\) permutations, then the particles are indistinguishable and \(\ket{\psi} \in \mathcal{H}_1^{\otimes N}\):
\[
    \ket{\psi} \xrightarrow{P} \ket{\psi^{\prime}} = P\ket{\psi} = e^{i \alpha} \ket{\psi}.
\]
Let's then analyze further the \textit{permutation group} of these transformations.

\subsection{Permutation Group}
The set of all possible permutations \(P_j\) of \(N\) elements defines the permutation group, wich respects the usual properties of a transformations group:
\begin{itemize}
    \item Closed under a \textit{composition operation};
    \item It has an \textit{inverse element} with respect to the composition law;
    \item It has a \textit{neutral element} with respect to the composition law;
    \item It resoects \textit{associativity} under the composition law.
\end{itemize}
So if we consider two permutations \(P_i\) and \(P_j\), then their consequent application (their composition), is still a permutation. Since there are \(N!\) way of permuting \(N\) elements, we conclude that the permutation group is made of \(N!\) elements.

We can study the simplest transformation of this group, the\textbf{elementary permutation} (or transposition) exchanging the \(i-\)th element with the \((i+1)-\)th one, in order to understand better this group and his properties. The elementary permutation is so important because it is the \textbf{generator} of this transformation group: we can write any other permutation by composing different elementary permutations. Thus \(\sigma_i\) with \(i = 1,\,\dots,\,N\) are the generators of the group.
Decomposing a oermutation in such way is not uniquely defined:
\[
    \sigma = \sigma_{\alpha_1} \sigma_{\alpha_2} \dots \sigma_{\alpha_N},
\]
there mey be different combinations of \((\alpha_1,\,\alpha_2,\,\dots ,\,\alpha_N)\) which give the same result. But it can be shown that a permutation made by an even (or odd) number of elementary permutations, will always be made of an even (or odd) number of elementary permutation. Furthermore, the number of transpositions defines the sign of the decomposition:
\[
    \mathrm{sgn}(\sigma) = \begin{dcases}
        +1, & \text{ for an even number of transpositions}, \\
        -1, & \text{ for an odd number of transpositions}.  \\
    \end{dcases}
\]
It can be shown that the generators of the permutation group satisfy the following properties:
\begin{enumerate}
    \item \(\sigma_i \sigma_j = \sigma_j \sigma_i\), \(\forall \vert i-j \vert \geq 2\);\footnote{The condition on the indexes is due to the fact that if \(\vert i-j \vert = 1\) then we are consequently applying the transposition on the same element: \(\sigma_i \sigma_{i+1}\) puts the \(i\)-th element in \(i+2\) slot and the \(i+2\)-th in the \(i+1\), while \(\sigma_{i+1}\sigma_i\) does the opposite \(i \to i+1\) and \(i+2 \to i\), clearly different transpositions.}
    \item \(\sigma_i \sigma_{i+1} \sigma_i = \sigma_{i+1} \sigma_i \sigma_{i+1}\);
    \item \(\sigma_i^2 = \mathbb{I}\).
\end{enumerate}

\section{Quantum Statistics}

If we consider a generic state \(\psi(1,2,\dots ,N) \in \mathcal{H}^{\otimes N}\), invariant under permutation up to a phase (which we assume permutation dependent)
\[
    \psi(1,2,\dots ,N) \to \psi(P(1,2,\dots ,N)) = e^{i \phi_P} \psi(1,2,\dots ,N).
\]
We know that any oermutation can be decomposed into a composition of transpositions
\[
    P = \sigma_{\alpha_1} \sigma_{\alpha_2} \dots \sigma_{\alpha_k},
\]
then the action of \(P\) on \(\psi\) is the same as the consequent application of the corresponding transpositions on \(\psi\):
\[
    \psi \to P \psi = \sigma_{\alpha_1} \sigma_{\alpha_2} \dots \sigma_{\alpha_k} \psi = e^{i\phi_1} e^{i\phi_2} \dots e^{i\phi_k} \psi = e^{i \phi_P} \psi
\]
since we can write the action of the \(j-\)th transposition on \(\psi\) as
\[
    \sigma_{\alpha_j} \psi = e^{i \phi_j} \psi.
\]
Thus if a permutation is decomposed into its transpositions, the phase of the permutation is given by the sum of the phases of the transpositions, i.e.
\[
    \phi_P = \phi_1 + \phi_2 + \dots + \phi_k.
\]
We can now use the properties of the generators to exploit some properties of the phases of the transpositions:
\begin{enumerate}
    \item \(\sigma_i \sigma_j = \sigma_j \sigma_i \iff \phi_P = \phi_i + \phi_j = \phi_j + \phi_i\) clearly;
    \item \(\sigma_i \sigma_{i+1} \sigma_i = \sigma_{i+1} \sigma_i \sigma_{i+1} \iff  \phi_P = \phi_i + \phi_{i+1} + \phi_i = \phi_{i+1} + \phi_i + \phi_{i+1},\, \implies \phi_i = \phi_{i+1}\), so that every transposition has the same effect on the phase of the permutation invariant vector: \(\sigma_i \psi = e^{i \phi} \psi \forall i\);
    \item \(\sigma_i^2 = \mathbb{I} \iff (e^{i\phi})^2 = 1\), which is true for two values of \(\phi\): \(\phi = 0,\, \phi = \pi\).
\end{enumerate}
The last observation is the most important, giving powerful insights into the system's symmetry: \((e^{i \phi})^2 = 1 \implies e^{i \phi} = \pm 1\), so that each transposition gives a factor of 1 \((\phi = 0)\) or -1 \((\phi = \pi)\). But since the phase is the same for all transposition, then we have two fixed case:
\begin{itemize}
    \item \(\phi=0\) in which every transposition leaves the state totally invariant: it is a completely symmetric system made of \textbf{Bosons}:
          \[
              \begin{dcases}
                  \sigma_i \, \colon \, \psi \to \psi; \\
                  P \, \colon \, \psi \to \psi.
              \end{dcases}
          \]
    \item \(\phi=\pi\) in which every transposition changes the sign of the state: it is a completely antisymmetric system made of \textbf{Fermions}:
          \[
              \begin{dcases}
                  \sigma_i \, \colon \, \psi \to -\psi; \\
                  P \, \colon \, \psi \to \mathrm{sgn}(P)\psi.
              \end{dcases}
          \]
\end{itemize}

\subsection{Two Particle State}

If we consider \(N\) identical particles living in their generical \(d\)-dimensional Hilbert space, the total space is given by
\[
    \mathcal{H} = \mathcal{H}_1^{\otimes N} = \mathcal{L}^2(\mathbb{R}^{dN}).
\]
Let us restrict the case to \(N=2\), then \(\mathcal{H} = \mathcal{H}_1^{\otimes 2} = \mathcal{L}^2(\mathbb{R}^{2d})\). So we can consider states composed by the singular substates of the two partcles such that, if they are \textbf{distinguishable}
\[
    \psi_1(x_1),\, \psi_2(x_2) \implies \psi = \psi_1(x_1) \psi_2(x_2) \neq \psi_1(x_2)\psi_2(x_1) \in \mathcal{H}^{\otimes 2}.
\]
In general the last equation does not hold, they are different states, but if we consider \textbf{undistinguishable} paticles then we have \textit{total symmetry} or \textit{total antisymmetry} depending on the particles' nature.

We can introduce two operators, \(S\) and \(A\) which project our state onto the totally antisymmetric or symmetric subspaces as follows:
\[
    \begin{aligned}
        S \psi & = \psi_S (x_1,\,x_2) = \frac{\psi(x_1,\,x_2) + \psi(x_2,\,x_1)}{2} = \frac{\psi_1(x_1)\psi_2(x_2) + \psi_1(x_2)\psi_2(x_1)}{2} = \psi_S(x_2,\,\psi_1);   \\
        A \psi & = \psi_A (x_1,\,x_2) = \frac{\psi(x_1,\,x_2) - \psi(x_2,\,x_1)}{2} = \frac{\psi_1(x_1)\psi_2(x_2) - \psi_1(x_2)\psi_2(x_1)}{2} = - \psi_A(x_2,\,\psi_1).
    \end{aligned}
\]
Thus any state can be symmetrized or antisymmetrized with the action of these operators. Notice how, being projectors, these operators respect
\[
    \begin{aligned}
        S & = S^{\dagger},\quad S^2 = S; \\
        A & = A^{\dagger},\quad A^2 = A.
    \end{aligned}
\]
Another very important property is the orthogonality of these operators: they commute and any consequent application of both of them on a state is projected in the null space:
\[
    SA = AS = 0,\quad \bra{\psi_S}\ket{\psi_A} = \bra{\psi_A}\ket{\psi_S} = 0.
\]
Thus we can decompose our general Hilbert space into two direct sum subspaces:
\[
    \mathcal{H} = \mathcal{H}_S \oplus_{\bot} \mathcal{H}_A, \quad \implies \quad \psi = \psi_S + \psi_A.
\]
Thus the two subspaces are orthogonal and they share only the null vector.

\paragraph{Pauli exclusion principle.}
It's interesting to notice how the important \textbf{Pauli exclusion principle} arise from here, only by applying statistical prescription: if we consider two fermions in the same state
\[
    \psi(x_1,x_2) = \psi_1(x_1)\psi_1(x_2),
\]
we know this state to be antisymmetric, so if we apply the \(A\) operators we are not changing the state nor projecting it into a subspace, it's the same; then by applying it we notice how the wavefunction vanishes:
\[
    A \psi(x_1,\,x_2) = \frac{\psi_1(x_1)\psi_1(x_2) - \psi_1(x_2)\psi_1(x_1)}{2} = 0.
\]
Thus the principle: \textit{two fermions cannot occupy the same phyisical state}.

\subsection{N Particle State}

For a generic number of indistinguishable particle, we had \(\mathcal{H} = \mathcal{H}_1^{\otimes N}\), with states of the form \(\psi(x_1,\,\dots,\,x_N) = \psi_1(x_1)\psi_2(x_2)\dots\psi_N(x_N)\), which can be written in a more convenient form
\[
    \ket{\psi} = \ket{\psi_1}_1 \ket{\psi_2}_2 \dots \ket{\psi_N}_N \in \mathcal{H}.
\]
The index \(\psi_j\) refers to a particle state, a set of quantum numbers defining it, while the index on \(\ket{}_j\) refers to the particle itself. Thus we can express the action of the symmetry projectors as:
\[
    \begin{aligned}
        S \colon \ket{\psi} & = \frac{1}{N!} \sum_{\sigma} \ket{\psi_{\sigma(1)}}_1\ket{\psi_{\sigma(2)}}_2\dots\ket{\psi_{\sigma(N)}}_N \in \mathcal{H}_S;                      \\
        A \colon \ket{\psi} & = \frac{1}{N!} \sum_{\sigma} \mathrm{sgn}(\sigma) \ket{\psi_{\sigma(1)}}_1\ket{\psi_{\sigma(2)}}_2\dots\ket{\psi_{\sigma(N)}}_N \in \mathcal{H}_A.
    \end{aligned}
\]
These operators still respects the properties of projectors and they still commute: we can decompose the Hilbert space in:
\[
    \mathcal{H} = \mathcal{H}_S \oplus_{\bot} \mathcal{H}_A \oplus_{\bot} \mathcal{H}^{\prime},
\]
where the last subspace \(\mathcal{H}^{\prime}\) contains non phyisical states and solutions: we can think and write of a neither symmetric nor antisymmetric state, but it is non phyisical as all particles are fermions or bosons.

We will work in a space where we consider a variable number of particles, as we will describe in detail in the next chapter, direct sum of the spaces for every possible number of particle:
\[
    \mathcal{H}_{S / A} = \bigoplus_{N=0}^{\infty} \mathcal{H}_{S / A}^{(N)},
\]
aiming now to the intrinsic description of each hilbert space for a fixed \(N\): \(\mathcal{H}_{S / A}^{(N)} = \mathcal{H}_1^{\otimes N}\). We need an arbitrary ON basis, which we will construct from the basis of the single particle space:
\[
    \mathcal{H}_1 \colon \{\ket{\psi_n}\}_{n=1}^{\infty}.
\]
Thus if we combine \(N\) of this spaces (and basis), we find a basis which spans the whole \(N\) particle space \(\mathcal{H}^{\otimes N} = \mathcal{H}_{S / A}^{(N)}\):
\[
    \mathcal{H}_{S / A}^{(N)} \colon \{\ket{\psi_{\alpha_1} }\ket{\psi_{\alpha_2}}\dots\ket{\psi_{\alpha_N}}\}_{\alpha_1,\,\alpha_2,\,\dots,\,\alpha_N=1}^{\infty}.
\]

Remembering that we are speaking of undistinguishable particles, we can now expose a new description of our states: since we are not interested in the position and state of each particle, we consider just the states and their population, using the \textbf{occupation number} \(n_k\): we have an infinite amount of occupation numbers (infinite dimensionality of Hilbert space for one particle), each of whose can assume a value between 0 and \(N\):
\[
    0 < n_1,\,\dots,\,n_k,\,\dots < N, \quad n_1 + \dots + n_k + \dots = N,
\]
their sum has to be equal to the number of particles.

We can express a state
\[
    \psi_{n_1,\,\dots,\,n_k,\,\dots}(x_1,\,x_2,\,\dots,\,x_N)
\]
in the base \(\{\ket{\psi_{\alpha_1} }\ket{\psi_{\alpha_2}}\dots\ket{\psi_{\alpha_N}}\}_{\alpha_1,\,\alpha_2,\,\dots,\,\alpha_N=1}^{\infty}\) by setting accordingly the pedices, such that \(n_1\) pedices will be equal to one (\(\alpha_j = 1\)), \(n_k\) pedices will be equal to \(k\) (\(\alpha_j = k\)) eccetera.

This occupation numbers behave differently with respect to the particles' nature:
\[
    \begin{aligned}
        \text{Bosons: }   & n_k = 0,\,1,\,2,\,\dots,\,\infty; \\
        \text{Fermions: } & n_k = 0,\,1;
    \end{aligned}
\]
this is due to the Pauli exclusion principle: there cannot exist two fermions in the same state, while bosons do not have such kind of restrictions over occupation numbers.