\section{Definition of a Group}

Let us define a group \(G = \{g\}\) as a set of elements \(g\) that satisfy the following properties:

\begin{enumerate}
    \item \textbf{Composition law}: Given \(g_1, g_2 \in G\), then \(g_1 \cdot g_2 = g_3 \in G\).
    \item \textbf{Identity element}: There exists \(e \in G\) such that \(g \cdot e = e \cdot g = g\), for all \(g \in G\).
    \item \textbf{Inverse element}: For each \(g \in G\), there exists \(g^{-1} \in G\) such that \(g \cdot g^{-1} = g^{-1} \cdot g = e\).
    \item \textbf{Associativity}: \((g_1 \cdot g_2) \cdot g_3 = g_1 \cdot (g_2 \cdot g_3)\) for all \(g_1, g_2, g_3 \in G\).
\end{enumerate}

Groups are fundamental mathematical structures that describe symmetries in various physical systems. The elements of a group can represent transformations, such as rotations or reflections, that leave certain properties of a system invariant.

\subsection{Types of Groups}

We can classify groups into different types based on their properties and structure.
\begin{itemize}
    \item \textbf{Discrete groups}: Contain a finite number of elements. For example, the group \(Z_2 \equiv \{1, -1\}\) with the usual multiplication law defines a group with two elements.

    \item \textbf{Lie groups}: Groups with an infinite number of elements, where the elements depend continuously on certain parameters. For example, rotations around the \(z\)-axis of our three-dimensional space form a Lie group whose elements are parameterized by an angle \(\theta \in [0, 2\pi]\). For Lie groups, one can consider infinitesimal transformations leading to the concept of Lie algebras, which are essential in understanding the structure of the groups.

    \item \textbf{Abelian groups}: Groups whose elements commute under the composition law: \(g_1 \cdot g_2 = g_2 \cdot g_1\) for every \(g_1, g_2 \in G\). If this does not happen, the group is said to be \textbf{non-abelian}.
\end{itemize}

\begin{example}[Discrete groups]
    \begin{itemize}
        \item The \textbf{cyclic group} \(Z_n\): The finite group generated by the powers of an element \(a\), \(Z_n = \{e, a, a^2, \ldots, a^{n-1}\}\), with the condition that \(a^n = a^0 = e\). It is isomorphic to the \(n\)-th roots of unity \(e^{\frac{2\pi i}{n}k}\), with \(k = 0, 1, \ldots, n-1\). It is an abelian group for any \(n\).

        \item The \textbf{symmetric group} \(S_n\): The group of permutations of \(n\) objects, containing \(n!\) elements. One can check that \(S_2 = Z_2\), while \(S_3\) contains six elements and is the simplest example of a non-abelian group.
    \end{itemize}
\end{example}

\begin{example}[Lie groups]
    \begin{itemize}
        \item \(\mathrm{GL}(N,\mathbb{R}) = \left\{ g \in \mathbb{R}^{N \times N} \mid \det g \neq 0 \right\}\) \textbf{general linear group}: the group of real \(N \times N\) matrices with determinant \(\neq 0\).
        \item \(\mathrm{SL}(N,\mathbb{R}) = \left\{ g \in \mathrm{GL}(N,\mathbb{R}) \mid \det g = 1 \right\}\) \textbf{special linear group}: the group of real \(N \times N\) matrices with determinant \(= 1\).
        \item \(\mathrm{O}(N) = \left\{ g \in \mathrm{GL}(N,\mathbb{R}) \mid g^T g = \mathbb{I} \right\}\) \textbf{orthogonal group}: the group of real orthogonal \(N \times N\) matrices. It describes the invariances of the scalar product \(x^T x\) with \(x \in \mathbb{R}^N\).
        \item \(\mathrm{SO}(N) = \left\{ g \in \mathrm{O}(N) \mid \det g = 1 \right\}\) \textbf{special orthogonal group}: the group of real orthogonal \(N \times N\) matrices with determinant \(= 1\).
        \item \(\mathrm{GL}(N,\mathbb{C}) = \left\{ g \in \mathbb{C}^{N \times N} \mid \det g \neq 0 \right\}\) \textbf{general linear group}: the group of complex \(N \times N\) matrices with determinant \(\neq 0\).
        \item \(\mathrm{SL}(N,\mathbb{C}) = \left\{ g \in \mathrm{GL}(N,\mathbb{C}) \mid \det g = 1 \right\}\) \textbf{special linear group}: the group of complex \(N \times N\) matrices with determinant \(= 1\).
        \item \(\mathrm{U}(1) = \{z \in \mathbb{C} \mid |z| = 1\} = \{e^{i\theta} \mid \theta \in [0, 2\pi]\}\) \textbf{unitary group}: the group of phases. It describes the invariances of the product \(z^* z\) with \(z \in \mathbb{C}\).
        \item \(\mathrm{U}(N) = \left\{ g \in \mathrm{GL}(N,\mathbb{C}) \mid g^\dagger g = \mathbb{I} \right\}\) \textbf{unitary group}: the group of unitary \(N \times N\) matrices. It describes the invariances of the scalar product \(z^\dagger z\) with \(z \in \mathbb{C}^N\).
        \item \(\mathrm{SU}(N) = \left\{ g \in \mathrm{U}(N) \mid \det g = 1 \right\}\) \textbf{special unitary group}: the group of unitary \(N \times N\) matrices with determinant \(= 1\).
    \end{itemize}
    There are important relationships between these groups, for example:
    \[
        \begin{aligned}
            U(1) & \cong SO(2),          \\
            O(N) & = Z_2 \otimes SO(N),  \\
            U(N) & = U(1) \otimes SU(N).
        \end{aligned}
    \]
    These isomorphisms and decompositions reveal the underlying structure of these symmetry groups and are fundamental in many physical applications.
\end{example}

\section{Representations} \label{sec:representations}

We now introduce the concept of group representation. A \textit{representation} of an abstract group \(G\) is a ``realization'' of the multiplicative relations of the group \(G\) in a corresponding group of square matrices, where the product is given by the usual matrix multiplication. These matrices must be thought of as \textit{linear operators} that act on a \textit{vector space} \(V\), whose dimension is called the \textit{dimension of the representation}.

Explicitly, a representation is given by a mapping:
\[
    \begin{aligned}
        R: G & \longmapsto \text{Square Matrices}, \\
        g    & \longmapsto R(g),
    \end{aligned}
\]
such that:
\begin{enumerate}
    \item \(R(g_1) R(g_2) = R(g_1 \cdot g_2)\).
    \item \(R(e) = \mathbb{I}\), where \(\mathbb{I}\) is the identity matrix.
\end{enumerate}
From these properties, it also follows that \(R(g^{-1}) R(g) = R(e) = \mathbb{I}\), hence \(R(g^{-1}) = R^{-1}(g)\) exists. Associativity is automatic because matrix multiplication is associative. Thus, all the properties of the group are explicitly realized by the matrices of a representation.

By thinking of the matrices of a representation as operators that act on a vector space \(V\) of dimension \(N\), the matrices are \(N \times N\) matrices, and that is why the representation is said to be of dimension \(N\).

\begin{example}[Representation of \(Z_2\)]
    As a very simple example of a representation, consider the cyclic group \(Z_2 = \{e, a\}\) with the relation that \(a^2 = e\). Then, a simple two-dimensional representation is given by the following \(2 \times 2\) matrices:
    \[
        R(e) = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}, \quad R(a) = \begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}.
    \]
    It is easily checked that the matrices of this representation satisfy all the properties of the abstract group \(Z_2 = \{e, a\}\). As we shall understand soon, this representation is reducible, as it contains two copies of the more simple representation defined in terms of \(1 \times 1\) matrices, i.e., numbers:
    \[
        R(e) = 1, \quad R(a) = -1.
    \]
\end{example}

\subsection{Classification of Representations}

At this point, the problem arises of studying how many and what kinds of representations of a given group possibly exist. In particular, it is useful to know which are their dimensions. This problem is of great importance for physical applications because the ``vectors'' of a representation (generically called ``tensors'') are conveniently used to describe physical quantities associated with models where \(G\) acts as a symmetry group.

\textbf{Defining Representation}

In the list of Lie groups introduced in the previous examples, we have used matrices to define the groups. Thus, these matrices give rise immediately to a particular representation: the \textit{defining representation} (also called the \textit{fundamental representation}).

The elements of the group in the defining representation naturally perform transformations on vectors belonging to a vector space \(V\), the space on which the matrices act as linear operators. Let \(v^a\) denote the components of a vector in \(V\). The matrix \(R(g)\), which represents the element \(g\) of the abstract group \(G\), transforms this vector as follows:
\[
    v^a \quad \xrightarrow{g \in G} \quad v'^a = [R(g)]^a_{\ b} v^b,
\]
where, as usual, \([R(g)]^a_{\ b}\) describes, as the indices \(a\) and \(b\) vary, the elements of the matrix \(R(g)\). The row index \(a\) is the first index and is conventionally placed in the upper position, and the column index \(b\) is the second index and is conventionally placed in the lower position.

In this way, the vectors in the vector space \(V\) are transformed by operations associated with the group \(G\). Repeated indices are summed over all their possible values, and the convention is used that in the sum, one index is in the upper position and the other one in the lower position.

\textbf{Equivalent Representations}

In general, \textit{equivalent} representations are defined as those that are related by similarity transformations: \(R(g)\) and \(\tilde{R}(g)\) are equivalent representations if:
\[
    \tilde{R}(g) = A R(g) A^{-1} \quad \forall g \in G,
\]
where \(A\) is a matrix independent of \(g\). This equivalence relation allows us to consider equivalent representations as essentially the same representation. Indeed, the similarity transformation simply represents a change of basis in the vector space \(V\): the matrices of the different equivalent representations identify the same linear operator expressed in different bases.

\textbf{Reducible and Irreducible Representations}

A \textit{reducible} representation is a representation equivalent to a representation whose matrices are block diagonal. For example, \(R(g)\) is reducible if:

\[
    \tilde{R}(g) = A R(g) A^{-1} = \left(\begin{array}{c|c|c}
            R_1(g) & 0      & 0      \\
            \hline
            0      & R_2(g) & 0      \\
            \hline
            0      & 0      & R_3(g)
        \end{array}\right) \quad \forall g \in G,
\]
for an appropriate matrix \(A\). It is said that \(R(g)\) is reducible to the three representations \(R_1(g)\), \(R_2(g)\), \(R_3(g)\). In this example, the vector space \(V\) on which the reducible representation \(R(g)\) acts is naturally decomposed as a direct sum of the three vector spaces on which the representations \(R_1(g)\), \(R_2(g)\), \(R_3(g)\) act, i.e., \(V = V_1 \oplus V_2 \oplus V_3\). This reducibility is thus written as:
\[
    R(g) = R_1(g) \oplus R_2(g) \oplus R_3(g).
\]
An \textit{irreducible} representation is a representation that cannot be decomposed as above\footnote{For some groups, there can exist reducible representations of a particular type, formed by upper triangular matrices, but we overlook this subtlety in a first exposition to group theory, as the simplest groups we are interested in do not show such a phenomenon.}.

In the classification of the possible representations of a group \(G\), it is useful to consider only inequivalent irreducible representations, as all other representations follow from them. Given a fixed integer \(N\), it is not guaranteed that an irreducible representation of dimension \(N\) exists. In general, only for certain values of \(N\) will there be representations of a fixed group \(G\) (sometimes even more than one with the same dimension).

\textbf{Unitary Representations}

A \textit{unitary} representation is a representation in terms of unitary matrices (operators). Unitary representations are very useful in applications of quantum mechanics, where the symmetries of a quantum system are described by unitary operators acting in the Hilbert space (an infinite-dimensional vector space endowed with a positive-definite norm).

\textbf{Derived Representations}

Given the defining representation \(R(g)\) that acts on the vector space \(V\), which corresponds to transforming vectors with upper indices, we can immediately construct three other representations:
\begin{itemize}
    \item \(R(g)^*\), the \textbf{complex conjugate} representation acting on \(V^*\).
    \item \(R(g)^{-1T}\), the \textbf{inverse transposed} representation acting on the dual space \(\tilde{V}\).
    \item \(R(g)^{-1\dagger}\), the \textbf{inverse Hermitian conjugate}\footnote{Given a matrix \(R\), its Hermitian conjugate (or adjoint) \(R^\dagger\) is defined as the complex conjugate of the transpose, \(R^\dagger = R^{T*}\).} representation acting on \(\tilde{V}^*\).
\end{itemize}
We could not have an inverse representation without transposing, since when taking the inverse of a product of matrices, the order of the matrices is reversed:
\[
    ((AB)^{-1})^T = (B^{-1} A^{-1})^T = A^{-1T} B^{-1T},
\]
and the same is true for the inverse Hermitian conjugate representation (again the order of the matrices is reversed two times to ensure the correct properties of the representation).

The vectors on which these representations act have follow a precise index structure by convention\footnote{More information about dotted indices can be found in Appendix \ref{app:indices}.}, respectively:
\begin{itemize}
    \item Vectors with ``upper indices'' \(v^a\) (vectors in the space \(V\)).
    \item Vectors with ``dotted upper indices'' \(v^{\dot{a}}\) (vectors in the complex conjugate space \(V^*\)).
    \item Vectors with ``lower indices'' \(v_a\) (vectors in the dual space \(\tilde{V}\)).
    \item Vectors with ``dotted lower indices'' \(v_{\dot{a}}\) (vectors in the complex conjugate dual space \(\tilde{V}^*\)).
\end{itemize}
In formulae:
\begin{equation}
    \begin{aligned}
        R(g) : \quad  v^a                    & \xrightarrow{g \in G} v'^a = [R(g)]^a_{\ b} v^b,                                                        \quad v \in V       \\
        R(g)^* : \quad v^{\dot{a}}           & \xrightarrow{g \in G} v'^{\dot{a}} = [R(g)^*]^{\dot{a}}_{\ \dot{b}} v^{\dot{b}}, \quad v \in V^{\ast}                       \\
        R(g)^{-1T} : \quad v_a               & \xrightarrow{g \in G} v'_a = [R(g)^{-1T}]_a^{\ b} v_b,                                                \quad v \in \tilde{V} \\
        R(g)^{-1\dagger} : \quad v_{\dot{a}} & \xrightarrow{g \in G} v'_{\dot{a}} = [R(g)^{-1\dagger}]_{\dot{a}}^{\ \dot{b}} v_{\dot{b}}, \quad v \in \tilde{V}^*.
    \end{aligned}
\end{equation}
It is immediate to verify that these are representations of the group \(G\) if \(R(g)\) is one. The different index structure associated with these matrices reflects the fact that they are operators acting on different vector spaces.

\begin{remark}
    Note that there may be groups for which some of these representations are equivalent to each other:
    \begin{itemize}
        \item For real representations, \(R(g)^* \cong R(g)\) and \(R(g)^{-1\dagger} \cong R(g)^{-1T}\).
        \item For unitary representations, \(R(g)^{-1\dagger} \cong R(g)\) and \(R(g)^{-1T} \cong R(g)^*\) (since \(R(g)^\dagger = R(g)^{-1}\)).
        \item For orthogonal representations, \(R(g)^{-1T} \cong R(g)\) and \(R(g)^{-1\dagger} \cong R(g)^*\) (since \(R(g)^T = R(g)^{-1}\)).
        \item For unitary and real representations, all four representations are equivalent to each other (for example, for the group \(SO(N)\)).
    \end{itemize}
\end{remark}

\subsection{Invariant Quantities and Index Contractions} \label{sec:invariant_contractions}

Invariant quantities under the action of the group \(G\) can be obtained by taking the scalar product between vectors with upper indices (sometimes called contravariant) and those with lower indices (sometimes called covariant), whether dotted or undotted. One can verify the following identities:
\[
    \begin{aligned}
        v_a w^a                 & \xrightarrow{g \in G} v'_a w'^a = v'^T w' = (R(g)^{-1T} v)^T R(g) w = v^T R(g)^{-1} R(g) w = v^T w = v_a w^a,                                            \\
        x_{\dot{a}} y^{\dot{a}} & \xrightarrow{g \in G} x'_{\dot{a}} y'^{\dot{a}} = x'^T y' = (R(g)^{-1\dagger} x)^T R(g)^* y = x^T R(g)^{-1*} R(g)^* y = x^T y = x_{\dot{a}} y^{\dot{a}}.
    \end{aligned}
\]

In general, it makes no group-theoretic sense to contract indices of the vectors described above in any other way (``contracting'' refers to the operation of equating two indices and summing over all possible values that these indices can assume). For example, the quantities \(v^a w_a\) or \(x^{\dot{a}} y_{\dot{a}}\) are not invariant under the action of the group \(G\), and contracting indices of different nature (for example, \(v^a y_{\dot{a}}\)) is not even defined.

\subsection{Tensors and Tensor Representations}

Other representations can be obtained from the tensor product of the previously described representations. By definition, these representations act on ``tensors,'' which are elements of vector spaces obtained from the tensor product of copies of \(V\), \(V^*\), \(\tilde{V}\), and \(\tilde{V}^*\). Therefore, tensors, by definition, have a certain number of upper and lower indices, with transformation properties defined by the nature associated with those indices.

For example, a tensor \(F^{ab \phantom{c} \dot{d}}_{\phantom{ab} c \phantom{d} \dot{e}}\) is, by definition, an object with \(N^5\) components that transform exactly like the product of the components of the previously defined vectors (tensor product):
\[
    F^{ab \phantom{c} \dot{d}}_{\phantom{ab} c \phantom{d} \dot{e}} \sim v^a u^b w_c x^{\dot{d}} y_{\dot{e}}.
\]
Thus, the tensor \(F^{ab \phantom{c} \dot{d}}_{\phantom{ab} c \phantom{d} \dot{e}}\) represents (the components of) an element of a vector space of dimension \(N^5\) (because each index can take \(N\) values; it corresponds to an element of the vector space \(V \otimes V \otimes \tilde{V} \otimes V^* \otimes \tilde{V}^*\) and we can write \(F^{ab \phantom{c} \dot{d}}_{\phantom{ab} c \phantom{d} \dot{e}} \in V \otimes V \otimes \tilde{V} \otimes V^* \otimes \tilde{V}^*\)).

\subsubsection{Tensor Transformation Law}

Under the action of the group \(G\), the tensor transforms as follows:
\[
    F'^{ab \phantom{c} \dot{d}}_{\phantom{\prime ab} c \phantom{d} \dot{e}} = [R(g)]^a_{\ f} [R(g)]^b_{\ g} [R(g)^{-1T}]_c^{\ h} [R(g)^*]^{\dot{d}}_{\ \dot{m}} [R(g)^{-1\dagger}]_{\dot{e}}^{\ \dot{n}} F^{fg \phantom{h} \dot{m}}_{\phantom{fg} h \phantom{m} \dot{n}}.
\]
This linear transformation law identifies a representation of dimension \(N^5\) (the \(N^5\) components are mixed among themselves by an \(N^5 \times N^5\) matrix, implicitly defined by the above formula, thus providing a representation of the group).

\subsubsection{Reducibility of Tensor Representations}

Typically, tensors correspond to reducible representations, i.e., are transformed by reducible representations. The problem of decomposing representations into irreducible ones now arises. One way to decompose a representation is to study the tensors on which they act. A first decomposition operation is to separate the tensors by considering their symmetry properties under permutations of indices of the same nature (it is, therefore, useful to know the properties of the permutation group of \(n\) objects, i.e., the symmetric group \(S_n\)).

For example, the tensor \(T^{ab}\) can be separated into its symmetric part (\(S^{ab} = S^{ba}\)) and its antisymmetric part (\(A^{ab} = -A^{ba}\)) as follows:
\[
    T^{ab} = \underbrace{\frac{1}{2}(T^{ab} + T^{ba})}_{S^{ab}} + \underbrace{\frac{1}{2}(T^{ab} - T^{ba})}_{A^{ab}}.
\]
It is easy to convince oneself that the symmetric and antisymmetric parts with distinct symmetries do not mix under group transformations. Indeed, one can calculate the transformed symmetric part under an arbitrary group transformation and verify that it remains symmetric:
\[
    \begin{aligned}
        S^{ab} \xrightarrow{g \in G} S'^{ab} & = [R(g)]^a_{\ c}[R(g)]^b_{\ d} S^{cd}            \\
                                             & = [R(g)]^a_{\ c}[R(g)]^b_{\ d} S^{dc}            \\
                                             & = [R(g)]^b_{\ d}[R(g)]^a_{\ c} S^{dc} = S'^{ba}.
    \end{aligned}
\]

Similarly, one can verify that the antisymmetric part remains antisymmetric:
\[
    \begin{aligned}
        A^{ab} \xrightarrow{g \in G} A'^{ab} & = [R(g)]^a_{\ c}[R(g)]^b_{\ d} A^{cd}              \\
                                             & = [R(g)]^a_{\ c}[R(g)]^b_{\ d} (-A^{dc})           \\
                                             & = -[R(g)]^b_{\ d}[R(g)]^a_{\ c} A^{dc} = -A'^{ba}.
    \end{aligned}
\]

Thus, symmetric parts and antisymmetric parts are never mixed by group transformations, so the tensor representation identified by the tensor \(T^{ab}\) is reducible. In a compact notation, we can denote the representation that transforms the tensor \(T^{ab} \sim T\) as \(R_T(g)\), by grouping all the tensorial indices of different nature and position with a single multi-index, so that:
\[
    \begin{aligned}
        T^{ab \phantom{c} \dot{d}}_{\phantom{ab} c \phantom{d} \dot{e}} & \to T^A, \quad T'^A = [R_T(g)]^A_{\ B} T^B,                                                                                   \\
        [R_T(g)]^A_{\ B}                                                & = [R(g)]^a_{\ k} [R(g)]^b_{\ f} [R(g)^{-1T}]_c^{\ l} [R(g)^*]^{\dot{d}}_{\ \dot{m}} [R(g)^{-1\dagger}]_{\dot{e}}^{\ \dot{n}}.
    \end{aligned}
\]
This representation is reducible:
\[
    \begin{pmatrix} S' \\ A' \end{pmatrix} = \underbrace{\begin{pmatrix} R_S(g) & 0 \\ 0 & R_A(g) \end{pmatrix}}_{R_T(g)} \begin{pmatrix} S \\ A \end{pmatrix},
\]
where \(T \sim \begin{pmatrix} S \\ A \end{pmatrix}\) indicates the decomposition into symmetric and antisymmetric parts.

These parts may be further reduced if there are other invariant operations (such as the possibility of taking scalar products of vectors and covectors in different representations as seen in \ref{sec:invariant_contractions}). For the simpler representations, it is easy to study any further reducibility on a case-by-case basis.

\subsubsection{Invariant Tensors}

Note that the Kronecker delta tensors \(\delta^a_{\ b}\) and \(\delta^{\dot{a}}_{\ \dot{b}}\), which are the matrix elements of the identity operators, remain invariant under group transformations if their indices are transformed according to their nature. For example:
\begin{equation}
    \begin{aligned}
        \delta^a_{\ b} \xrightarrow{g \in G} (\delta')^a_{\ b} & = [R(g)]^a_{\ c}[R(g)^{-1T}]_b^{\ d} \delta^c_{\ d} \\
                                                               & = [R(g)]^a_{\ c}[R(g)^{-1T}]_b^{\ c}                \\
                                                               & = [R(g)]^a_{\ c}[R(g)^{-1}]^c_{\ b}                 \\
                                                               & = [R(g)R(g)^{-1}]^a_{\ b} = \delta^a_{\ b}.
    \end{aligned}
    \label{eq:delta_invariant_tensor}
\end{equation}
These are called \textit{invariant tensors}. In contrast, \(\delta_{ab}\) does not identify any invariant tensor (unless there are special relations between the various types of indices): if we define a tensor that coincides with \(\delta_{ab}\) in a ``reference frame'', under a group transformation (a ``change of reference frame'') the components of the tensor change value.

The existence and number of invariant tensors depend on the group \(G\) under consideration. For example:
\begin{itemize}
    \item The group \(\mathrm{SO}(N)\) admits an invariant tensor defined by the completely antisymmetric symbol \(\epsilon^{a_1 \ldots a_N}\), where the indices are those of the fundamental representation. This follows from the unitary determinant of the matrices in \(\mathrm{SO}(N)\).
    \item Similarly, the group \(\mathrm{SU}(N)\) admits the invariant tensors given by the completely antisymmetric symbols \(\epsilon^{a_1 \ldots a_N}\) and \(\epsilon_{a_1 \ldots a_N}\).
\end{itemize}